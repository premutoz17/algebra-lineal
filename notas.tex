\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[mathscr,mathcal]{euscript}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{shapepar}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{shapepar}
\newtheorem{theorem}{Teorema}
\newtheorem{proposition}{Proposición}
\newtheorem{definition}{Definición}
\newtheorem{axiom}{Axioma}
\newtheorem{corollary}{Corolario}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark}

\title{Notas de Álgebra Lineal}
\author{Carlos Francisco Flores Galicia.}
\date{}

\begin{document}

\maketitle

\chapter{Espacios vectoriales.}
\subsection{Espacios vectoriales}
\subsection{Subespacios vectoriales}
\subsection{Combinaciones lineales}

\begin{definition}
Sea $V$ un espacio vectorial y $S \subseteq V$, $S \neq \emptyset$. Un vector $v \in V$ es combinación lineal de elementos de $S$, si existe un conjunto finito $\{s_1,s_2,...,s_n\}\subseteq S$ y escalares $a_1,a_2,...a_n \in K$ tal que $v=a_1 s_1+a_2 s_2+...+a_n s_n$. Se dice que $v$ es combinación lineal de $\{s_1,s_2,...,s_n\}$.
\end{definition}

\begin{definition}
Sea $V$ un espacio vectorial y $S \subseteq V$. El conjunto generado por $S$ se denota por $\langle S \rangle$, y es el conjunto de todas las combinaciones lineales formadas por los elementos de $S$.
\end{definition}

\begin{definition}
$\langle \emptyset \rangle=\{0_V\}$
\end{definition}

\begin{theorem}
Sea $V$ un espacio vectorial y $S \subseteq V$, $S \neq \emptyset$, entonces $\langle S \rangle \leq V$ y $\langle S \rangle$ es el subespacio de $V$ más pequeño que contiene a $S$ (es decir, que $\langle S \rangle$ es un subconjunto de todos los subespacios de $V$ que contienen a $S$).
\end{theorem}

\begin{proof}
Probemos primero que $\langle S \rangle \leq V$. Como $S \neq \emptyset$, al menos $0_V \in \langle S \rangle$. Luego, sean $u,v \in \langle S \rangle$, por tanto $u$ y $v$ son combinaciones lineales de elementos de $S$, de manera que existen $s_1,s_2,...s_n,t_1,t_2,...,t_n \in S$ tales que $v=a_1s_1+...+a_ns_n$ y $u=b_1t_1+...+b_nt_n$, con $a_1,...a_n,b_1,...,b_n \in K$. Ahora bien, es claro que $v+u=a_1s_1+...+a_ns_n+b_1t_1+...+b_nt_n$ y $cu=cb_1t_1+...+cb_nt_n$ pertenecen a $\langle S \rangle$, para cualquier $c \in K$. Por lo tanto $\langle S \rangle \leq V$.\newline \newline
Por otra parte, sea $U$ un subespacio de $V$ que contiene a $S$. Sea $v \in \langle S \rangle$, entonces $v=a_1s_1+...+a_ns_n$, con $a_1,...,a_n \in K$ y $s_1,...,s_n \in S$, además como $S \subseteq U$ entonces $v=a_1s_1+a_2s_2+...+a_ns_n \in U$, pues los subespacios vectoriales son cerrados bajo la suma y bajo el producto por escalares. Por tanto tenemos que si $v \in \langle S \rangle$ entonces $v \in U$, así que $\langle S \rangle \subseteq U$.
\end{proof}

\begin{definition}
Sea $S \subseteq V$. Decimos que $S$ genera a $V$ si $\langle S \rangle = V$. También podemos decir que los elementos de $S$ generan a $V$.
\end{definition}

\subsection{Dependencia e independencia lineal.}

\begin{definition}
Sea $S\subseteq V$. Decimos que $S$ es linealmente dependiente si existe $s \in S$ tal que $s \in \langle S-\{s\} \rangle$.
\end{definition}

\begin{theorem}
Sea $S=\{s_1,s_2,...,s_n\}\subset V$. $S$ es linealmente dependiente si y solo si existen $a_1,a_2,...,a_n \in K$ tal que $a_1s_1+a_2s_2+...+a_ns_n=0_V$ y $a_1,a_2,...,a_n $ no son todos cero.
\end{theorem}

\begin{proof}
$\Rightarrow$ Supongamos que $S$ es linealmente dependiente, entonces existe $s \in S$ tal que $s \in \langle S-\{ s \} \rangle$, por tanto existen $s_1,s_2,...,s_n \in S-\{s\}$ y los escalares $a_1,a_2,...,a_n \in K$ tales que $s=a_1s_1+a_2s_2+...+a_ns_n$. Al sumar $-s$ en ambos lados de la expresión anterior obtenemos $0_V=-s+a_1s_1+a_2s_2+...+a_ns_n$, con lo cual se garantiza que no todos los escalares que multiplican a los vectores son cero, pues $-1$ multiplica a $s$.\newline \newline
$\Leftarrow$ Supongamos que existen $a_1,a_2,...,a_n \in K$, no todos cero, tales que $a_1s_1+a_2s_2+...+a_ns_n=0_V$. Puesto que no todos los escalares son cero, supongamos sin perdida de generalidad que $a_1\neq 0$, por tanto podemos multiplicar en ambos lados de la igualdad anterior por el escalar $\dfrac{1}{a_1}$. En consecuencia obtenemos $s_1+\dfrac{a_2}{a_1}s_2+...+\dfrac{a_n}{a_1} s_n=0_V$. Luego, al sumar $-s_1$ y multiplicar por $-1$ en ambos lados nos queda que $s_1=\left(-\dfrac{a_2}{a_1}\right) s_2+...+\left( - \dfrac{a_n}{a_1}\right) s_n$, esto es, que $s_1 \in \langle S -\{ s_1 \} \rangle$. Por lo tanto $S$ es linealmente dependiente.
\end{proof}

\begin{definition}
Sea $S \subseteq V$. Decimos que $S$ es linealmente independiente si y solo si no es linealmente dependiente.
\end{definition}

Por la equivalencia lógica $(P \Leftrightarrow \exists x (Q \wedge S)) \Leftrightarrow (\neg P \Leftrightarrow \forall x (Q\Rightarrow \neg S))$, el teorema anterior es equivalente a la siguiente proposición que enunciaremos como corolario.

\begin{corollary}
Sea $S=\{s_1,s_2,...,s_n\}\subset V$. $S$ es linealmente independiente si y solo si para todo $a_1,a_2,...,a_n \in K$ tal que si $a_1s_1+a_2s_2+...+a_ns_n=0_V$ entonces $a_1,a_2,...,a_n$ son todos cero.
\end{corollary}

\begin{proof}
Se sigue del teorema anterior y de la equivalencia lógica $(P \Leftrightarrow \exists x (Q \wedge S)) \Leftrightarrow (\neg P \Leftrightarrow \forall x (Q\Rightarrow \neg S))$.
\end{proof}

\begin{proposition}
Si $S \subseteq V$ y $0_V \in S$, entonces $S$ es linealmente dependiente.
\end{proposition}

\begin{proof}

\end{proof}

\begin{theorem}
El conjunto $\emptyset$ es linealmente independiente
\end{theorem}

\begin{proof}
Supongamos que $\emptyset$ es linealmente dependiente, entonces existe $s \in \emptyset$ tal que $s \in \langle \emptyset-\{s\} \rangle$. Como $s \in \emptyset$ entonces por definición del conjunto vacío se cumple que $s\neq s$ lo cual es una contradicción. Por lo tanto el conjunto $\emptyset$ es linealmente independiente.
\end{proof}

\begin{lemma}
Si $V$ es un $K-$espacio vectorial y $R\subseteq S\subseteq V$, entonces $\langle R \rangle \subseteq \langle S \rangle$.
\end{lemma}

\begin{proof}
Supongamos que $R\subseteq S\subseteq V$, y sea $r \in \langle R \rangle$, por lo tanto existe un subconjunto finito $\{r_1,r_2,...,r_m \} \subseteq R$ y los escalares $a_1,a_2,...a_m \in K$ tal que $r=a_1r_1,a_2r_2,...,a_mr_m$. Como $R\subseteq S$, entonces $r_1,r_2,...,r_m \in S$, de manera que $r=a_1r_1,a_2r_2,...,a_mr_m \in \langle S \rangle$, por lo tanto $\langle R \rangle \subseteq \langle S \rangle$.

\end{proof}

\begin{theorem}
Sea $V$ un $K-$espacio vectorial y sean $S_1\subseteq S_2\subseteq V$. Si $S_1$ es linealmente dependiente entonces $S_2$ también lo es.
\end{theorem}

\begin{proof}
Supongamos que $S_1$ es linealmente dependiente, entonces existe $s \in S_1$ tal que $s \in \langle S_1-\{s\} \rangle$. Luego, como $S_1\subseteq S_2$ entonces $S_1-\{s\} \subseteq S_2-\{s \}$, y por el lema anterior $\langle S_1-\{s \} \rangle \subseteq \langle S_2-\{s \} \rangle$, por lo que $s \in \langle S_2-\{s\} \rangle$, luego $S_2$ es linealmente dependiente.
\end{proof}

\begin{corollary}
Sea $V$ un $K-$espacio vectorial y sean $S_1\subseteq S_2\subseteq V$. Si $S_2$ es linealmente independiente entonces $S_1$ también lo es.
\end{corollary}

\begin{proof}
La demostración se sigue de hacer la contrapositiva del teorema anterior.
\end{proof}

\subsection{Bases y dimensiones.}

\begin{definition}
Sea $\beta \subseteq V$. Decimos que $\beta$ es una base para $V$ si y solo si $\beta$ es linealmente independiente y $\langle \beta \rangle=V$.
\end{definition}

\begin{theorem}

Sea $V$ un espacio vectorial y $\beta\subseteq V$. Luego $\beta$ es una base para $V$ si y sólo si cada vector $v \in V$ puede ser expresado de manera única como una combinación lineal de vectores de $\beta$.

\end{theorem}

\begin{proof}

$\Rightarrow$ Supongamos que $\beta$ es una base para $V$. Sea $v \in V$, entonces $v \in \langle \beta \rangle$, así que existe un subconjunto finito $\{b_1,b_2,...,b_n\}\subseteq \beta$ y los escalares $a_1, a_2, ..., a_n \in K$ tal que $v = a_1 b_1 + a_2 b_2 + ...+ a_n b_n$. Ahora, supongamos que esta representación no es única, que también $v = c_1 b_1 + c_2 b_2 + ...+ c_n b_n$, con $c_1,c_2,...,c_n \in K$. Entonces es claro que $v-v=0_V=(a_1 - c_1)b_1+(a_2 -c_2)b_2 +...+(a_n -c_n)b_n$. Ya que $\beta$ es linealmente independiente, $a_1 -c_1=0, a_2 -c_2=0, ..., a_n -c_n=0$, en consecuencia $a_1=c_1, a_2 =c_2, ..., a_n =c_n$, por lo tanto la representación de $v$ como combinación lineal de $\beta$ es única.\newline

$\Leftarrow$ Supongamos que para cada vector $v \in V$ existe un subconjunto finito $\{b_1,b_2,...,b_n\} \subseteq \beta$ y los escalares únicos $a_1, a_2, ..., a_n \in K$, de tal forma que $v= a_1 b_1 + a_2 b_2 + ...+ a_n b_n$. Por lo tanto $\langle \beta \rangle=V$. Probemos ahora que $\beta$ es linealmente independiente. Tenemos que el elemento $0_V$ puede ser expresado como $0_V=a_1b_1+a_2b_2+...+a_nb_n$, y puesto que esta manera es única, se tiene que cada escalar $a_1, a_2, ..., a_n$ debe ser forzosamente $0$, por lo tanto $\beta$ es linealmente independiente, y en consecuencia $\beta$ es una base.

\end{proof}

\begin{theorem}
Si $V$ es un espacio vectorial y $S \subseteq V$ tal que $S$ es finito y genera a $V$, entonces existe $S' \subseteq S$ tal que $S'$ es una base para $V$.

\end{theorem}

\begin{proof}
Si $S= \emptyset$ o $S=\{0_V\}$ entonces $V=\{0_V\}$ y como $\emptyset$ es subconjunto de cualquier conjunto, entonces $S$ es una base para $V$. De lo contrario, $V$ tendrá al menos un elemento $v_1$ no nulo. Nótese que $\{v_1\}$ es un conjunto linealmente independiente. Continúese, si es posible, escogiendo elementos $v_2,v_3,...,v_r \in V$ tales que $\{ v_1,v_2,v_3,...,v_r \}$ sea linealmente independiente. Puesto que $S$ es finito, se llegará al punto en el que $S'=\{v_1,v_2,v_3,...,v_r\}$ sea un subconjunto de $S$ linealmente independiente, de manera que al agregar otro delemento de $S$ a $S'$, éste sea linealmente dependiente. Demostremos ahora que $S'$ es una base para $V$. Como $S'$ es linealmente independiente, basta mostrar que es generador de $V$, pero como $\langle S \rangle=V$, es suficiente demostrar que $S \subseteq \langle S' \rangle$. Sea $v \in S$. Si $v \in S'$, entonces $v \in \langle S' \rangle$. Por otro lado, si $v$ no está en $S'$, la anterior construcción mostraría que $S' \cup \{v\}$ es linealmente dependiente. Así, $v \in \langle S' \rangle$, y por tanto $S \subseteq \langle S' \rangle$.
\end{proof}

\begin{theorem}
Sea $V$ un espacio vectorial que tiene una base $\beta$ con exactamente $n$ elementos. Sea $S=\{s_1,s_2,...,s_m\} \subseteq V$ linealmente independiente con exactamente $m$ elementos, donde $m \leq n$. Entonces, existe un subconjunto $S_1 \subseteq \beta$ que contiene exactamente $n-m$ elementos tales que $\langle S \cup S_1 \rangle=V$. 
\end{theorem}

\begin{proof}
La demostración se hará por inducción sobre $m$. Si $m=0$, entonces $S=\emptyset$, y así $S_1=\beta$ satisface el teorema. Ahora, supongamos que que el teorema es cierto para $m$, tal que $m<n$, y demostremos que también se cumple para $m+1$. Sea $S=\{s_1,s_2,...,s_m,s_(m+1)\}$ un subconjunto de $V$ linealmente independiente, el cual contiene exactamente $m+1$ elementos. Puesto que $S=\{s_1,s_2,...,s_m\}$ también es linealmente independiente, por la hipótesis de inducción se tiene que existe un subconjunto $\{b_1,b_2,...,b_(n-m)\}$ de $\beta$ tal que $\{s_1,s_2,...,s_m\} \cup \{b_1,b_2,...,b_{n-m}\}$ genera a $V$. En consecuencia existirán escalares $a_1,...,a_m,c_1,c_2,...,c_{n-m} \in K$ tales que $y_{m+1}=a_1s_1+...+a_ms_m+c_1b_1+c_2b_2+...+c_{n-m}b_{n-m}$. Notemos que algún $b_i$, tal como $b_1$, es no nulo, de lo contrario $y_{m+1}$ es una combinación lineal de $S=\{s_1,s_2,...,s_m,s_(m+1)\}$ y eso contradice el hecho de que $S$ es linealmente independiente. Al despegar $b_1$ se obtiene $b_1=(-c_1^{-1}a_1)s_1+...+(-c_1^{-1}a_m)s_m-(-c_1^{-1})s_{m+1}+(-c_1^{-1}c_2)s_2+...+(-c_1^{-1}c_{n-m})s_{n-m}$. Entonces $b_1 \in \langle \{s_1,...,s_m,s_{m+1},b_2,...,b_{n-m} \} \rangle$, pero como $s_1,...,s_m,b_2,...,b_{n-m}$ son elementos de $ \langle \{s_1,...,s_m,s_{m+1},b_2,...,b_{n-m} \} \rangle$, se tendrá que $\{s_1,...,s_m,b_2,...,b_{n-m}\} \subseteq  \langle \{s_1,...,s_m,s_{m+1},b_2,...,b_{n-m} \} \rangle$. Por lo tanto $\langle \{s_1,...,s_m,s_{m+1},b_2,...,b_{n-m} \} \rangle = V$. Luego, al escoger $S_1=\{s_2,...,s_{n-m} \}$ demuestra que el teorema es cierto para $m+1$. Esto completa la demostración.
\end{proof}

\begin{corollary}
Sea $V$ un espacio vectorial que tiene una base $\beta$ que contenga exactamente $n$ elementos. Entonces, cualquier subconjunto linealmente independiente de $V$ que contenga exactamente $n$ elementos es una base de $V$.
\end{corollary}

\begin{proof}
Sea $S=\{s_1,s_2,..,s_n\}$ un conjunto de $V$ linealmente independiente que contiene exactamente $n$ elementos. Por el teorema anterior, existe $S_1 \subseteq \beta$ que contiene $n-n=0$ elementos tal que $\langle S \cup S_1\rangle =V$. Obviamente $S_1=\emptyset$; luego, $\langle S \rangle =V$. Como $S$ es también linealmente independiente, $S$ es una base para $V$.
\end{proof}

\begin{corollary}
Sea $V$ un espacio vectorial que tiene una base $\beta$ que contenga exactamente $n$ elementos. Entonces, cualquier subconjunto de $V$ que contenga más de $n$ elementos es linealmente dependiente.
\end{corollary}

\begin{proof}
Sea $S\subseteq V$ que contiene más de $n$ elementos. 	Supongamos que $S$ es linealmente independiente. Sea $S_1 \subset S$ con exactamente $n$ elementos . Entonces, por el corolario anterior $S_1$ es una base para $V$. Como $S_1$ es subconjunto propio de $S$, podemos tomar un elemento $s\in S$ tal que $s\notin S_1$. Como $S_1$ es base para $V$, $s\in \langle S_1 \rangle =V$. Luego, $S_1 \cup \{s\}$ es linealmente dependiente. Pero $S_1 \cup \{s\} \subseteq S$; luego, $S$ es linealmente dependiente, y esto es una contradicción. Por lo tanto, $S$ es linealmente dependiente.

\end{proof}

\begin{corollary}
Sea $V$ un espacio vectorial que tiene una base $\beta$ con exactamente $n$ elementos. Entonces, toda base para $V$ contendrá exactamente $n$ elementos.
\end{corollary}

\begin{proof}
Sea $S$ una base de $V$. Como $S$ es linealmente independiente tendrá como máximo $n$ elementos. Supongamos que $S$ contiene exactamente $m$ elementos; luego, $m\leq n$. Pero además, $S$ es una base de $V$ y $\beta$ es linealmente independiente. Entonces, aplicamos el corolario anterior intercambiando los papeles de $\beta$ y $\S$ para dar $n \leq m$. Luego $m=n$.
\end{proof}

\begin{definition}
Un espacio vectorial $V$ es dimensionalmente finito  si tiene una base cuya cardinalidad es un número finito. La cardinalidad de una base de $V$ es la dimension de $V$, y se denota por $dim(V)$. Si un espacio vectorial no es dimensionalmente finito, se llama dimensionalmente infinito.
\end{definition}

\begin{theorem}
Sea $U\leq V$ y $dim(V)=n$. Entonces, $W$ es dimensionalmente finito y $dim(W)\leq n$. Además, si $dim(W)=n$, entonces $W=V$.
\end{theorem}

\begin{proof}
Si $W=\{0_w\}$, $W$ es dimensionalmente finito y $dim(W)=0\leq n$. De otra manera, existe un elemento no nulo $w_1\in W$, y así $\{w_1\}$ es linealmente independiente. Continuando en esta forma, tómese elementos $w_1,w_2,...,w_k\in W$ tales que $\{w_1,w_2,...,w_k \}$ sea linealmente independiente. Este proceso debe terminar en una etapa donde $\{w_1,w_2,...,w_k \}$ sea linealmente independiente pero de manera que al añadir cualquier elemento de $W$ se tenga un conjunto linealmente dependiente. Entonces, $W$ tiene una base finita que contiene no más de $n$ elementos; esto es, $dim(W)\leq n$. Si $dim(W)=n$, entonces una base para $W$ seria un subconjunto de $V$ linealmente independiente que contuviera $n$ elementos, y esto implicaría que la base para $W$ es también una base para $V$, por tanto $W=V$.
\end{proof}

\chapter{Matrices y transformaciones lineales.}
\subsection{Matrices}
\subsection{Transformaciones lineales.}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales sobre un campo $K$. Una función $T:V\rightarrow W$ se llama transformación lineal de $V$ en $W$ si para todo $u,v\in V$ y para todo $c\in K$, se cumple que $T(u+v)=T(u)+T(v)$ y $T(cv)=cT(v)$. Si $V=W$, a la transformación lineal se le llama operador lineal.

\end{definition}

\begin{proposition}

Sean $u,v\in V$ y $c\in K$. Luego, $T:V\rightarrow W$ es una transformación lineal si y solo si $T(cu+v)=cT(u)+T(v)$.

\end{proposition}

\begin{proof}

Como $T$ es transformación lineal, entonces $T(cu+v)=T(cu)+T(v)=cT(u)+T(v)$. La segunda implicación es análoga a la primera.

\end{proof}

\begin{proposition}

Si $T:V\rightarrow W$ es una transformación lineal, entonces $T(0_V)=0_W$.

\end{proposition}

\begin{proof}
\[ 
\begin{split}
0_V &=0_V+0_V \\
T(0_V) &= T(0_V+0_V) \\
T(0_V) &= T(0_V)+T(0_V) \\
\end{split}
\]

Por otra parte, como $T(0_V)\in W$, entonces $T(0_V)=T(0_V)+0_W$. Luego
\[ 
\begin{split}
T(0_V)+T(0_V) &= T(0_V)+0_W \\
T(0_V) &= 0_W
\end{split}
\]

\end{proof} 

\begin{definition}
Sean $V$ y $W$ espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. El espacio nulo o kernel de $T$ es  
\[ker(T)=\{x\in V \ |\ T(x)=0_W \}\]
\end{definition}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. La imagen de $T$ es un subconjunto de $W$ que se define como 
\[Im(T)=\{T(x) \ |\ x\in V \}\]
\end{definition}

\begin{theorem}
Si $V$ y $W$ son espacios vectoriales y $T:V\rightarrow W$ una transformación lineal, entonces $ker(T)$ y $Im(T)$ son subespacios de $V$ y $W$, respectivamente.
\end{theorem}

\begin{proof}
Como $T(0_V)=0_W$, tenemos que $0_V\in ker(T)$. Sean $v_1,v_2 \in ker(T)$ y $c\in K$. Entonces $T(v_1+v_2)=T(v_1)+T(v_2)=0_W+0_W=0_W$, y $T(cv_1)=cT(v_1)=c0_W=0_w$. Por lo tanto $v_1,v_2\in ker(T)$ y $cv_1\in ker(T)$. Luego $ker(T)\leq V$

Por otra parte, Como $T(0_V)=0_W$, tenemos que $0_W\in Im(T)$. Ahora, sean $w_1,w_2 \in Im(T)$ y $c\in K$. Entonces existen $v_1$ y $v_2$ tales que $T(v_1)=w_1$ y $T(v_2)=w_2$. Así, $T(v_1+v_2)=T(v_1)+T(v_2)=w_1+w_2$, y $T(cv_1)=cT(v_1)=cw_1$. Por lo tanto, $v_1,v_2\in Im(T)$ y $cv_1\in Im(T)$. Luego $Im(T)\leq W$.
\end{proof}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. La nulidad de $T$ es $nul(T)=dim(ker(T))$.
\end{definition}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. El rango de $T$ es $R(T)=dim(Im(T))$.
\end{definition}

\begin{theorem}
Si $V$ y $W$ son espacios vectoriales, $V$ de dimension finita y $T:V\rightarrow W$ una transformación lineal, entonces $nul(T)+R(T)=dim(V)$.
\end{theorem}

\begin{proof}
Supongamos que $dim(V)=n$, y sea $\{b_1,b_2,...,b_k\}$ una base para $nul(T)$. Entonces podemos extender a $\{b_1,b_2,...,b_k\}$ para que sea una base para $V$. Supongamos que esa extensión es $\beta=\{b_1,b_2,...,b_n\}$. Demostraremos que el conjunto $S=\{T(b_{k+1}),...,T(b_n)\}$ es una base para $R(T)$.

Primero demostremos que $S$ genera a $R(T)$. 
\end{proof}

\begin{corollary}
Sean $V$ y $W$ espacios vectoriales y sea $T:V\rightarrow W$ una transformación lineal. Si $V$ tiene una base $\beta$, entonces $Im(T)=\langle T(\beta)\rangle$.
\end{corollary}

\begin{proof}
Se sigue de la demostración del teorema anterior. 
\end{proof}

\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. La transformación $T$ es inyectiva si y solo si $ker(T)=\{0_V\}$
\end{theorem}

\begin{proof}
Supongamos que $T$ es inyectiva y que $v\in ker(T)$. Entonces $T(v)=T(0_V)=0_W$, y por lo tanto $v=0_V$. Así, $ker(T)=\{0_V\}$. Ahora supongamos que $ker(T)=\{0_V\}$ y que $T(v_1)=T(v_2)$. Entonces $T(v_1)-T(v_2)=T(v_1-v_2)=0_W$, por lo tanto $x-y\in ker(T)=\{0_V\}$ y en consecuencia $x-y=0_V$, esto es, $x=y$. Por lo tanto $T$ es inyectiva.
\end{proof}

\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales de dimensiones finitas e iguales y $T:V\rightarrow W$ una transformación lineal. Entonces $T$ es inyectiva si y solo si es sobreyectiva.
\end{theorem}

\begin{proof}
$\Rightarrow$ Supongamos que $T$ es inyectiva, entonces $ker(T)=\{0_V\}$, por lo tanto $\emptyset$ es una base para $ker(T)$, y así $nul(T)=0$, por lo tato $0+R(T)=dim(V)=dim(W)$, luego $dim(Im(T))=dim(W)$, y en consecuencia $Im(T)=W$.\newline 
$\Leftarrow$ Supongamos que $T$ es sobreyectiva pero no es inyectiva. Entonces $nul(T)>0$, pues $\emptyset$ no es base para $ker(T)$. Como $T$ es sobreyectiva, entonces $Im(T)=W$, y así $R(T)=dim(W)=dim(V)$. Luego, $nul(T)+R(T)>dim(V)$, lo cual es una contradicción, por lo tanto $T$ es inyectiva.
\end{proof}

\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. Entonces $T$ es inyectiva si y solo si $T$ lleva subconjuntos linealmente independientes de $V$ a subconjuntos linealmente independientes de $W$.
\end{theorem}

\begin{proof}

\end{proof}

\begin{theorem}
Sean $V$ y $W$ espacios vectoriales y supóngase que $\{b_1,b_2,...,b_n\}$ es una base para $V$. Para cualquier subconjunto $\{w_1,w_2,...,w_n\}\subset W$ existe exactamente una transformación lineal $T:V\rightarrow W$ tal que $T(b_i)=w_i$ para $i=1,2,...,n$.
\end{theorem}

\begin{proof}

\end{proof}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales. Decimos que $V$ es isomorfo a $W$, si existe una transformación lineal $T:V \rightarrow W$ que sea invertible. Tal transformación lineal se llama isomorfismo de $V$ a $W$.
\end{definition}

\begin{proposition}
Si $V$ y $W$ son espacios vectoriales y $T:V\rightarrow W$ una transformación lineal con inversa $T^{-1}$, entonces $T^{-1}$ también es una transformación lineal.
\end{proposition}

\begin{proof}
Sean $w_1,w_2 \in W$ y $c\in K$. Cómo $T$ es biyectiva, existen los vectores únicos $v_1,v_2 \in V$ tales que $T(v_1)=w_1$ y $T(v_2)=w_2$. Entonces $T^{-1}(w_1)=v_1$ y $T^{-1}(w_2)=v_2$, así
\[
\begin{split}
T^{-1}(cw_1+w_2) &= T^{-1}(cT(v_1)+T(v_2)) \\
&= T^{-1}(T(cv_1+v_2)) \\
&= cv_1+v_2 \\
&= cT^{-1}(w_1)+T^{-1}(w_2)
\end{split}
\]
\end{proof}

\begin{theorem}
Sean $V$ y $W$ $K-$espacios vectoriales de dimensiones finitas. El espacio $V$ es isomorfo a $W$ si y solo si $dim(V)=dim(W)$.
\end{theorem}

\begin{proof}
$\Rightarrow$ Supongamos que $V$ es isomorfo a $W$, y que $\beta$ es una base para $V$. Entonces existe una transformación $T:V\rightarrow W$ biyectiva, y por tanto, invertible. Puesto que $T$ es inyectiva, entonces $T(\beta)$ es un subconjunto linealmente independiente de $W$. Luego, $T$ es sobreyectiva, entonces $\langle T(\beta)\rangle =Im(T)=W$, por lo que $T(\beta)$ es una base para $W$, así $dim(V)=dime(W)$.

$\Leftarrow$ Supongamos que $dim(V)=dim(W)$. Sean $\beta=\{b_1,b_2,...,b_n\}$ y  
$\alpha=\{a_1,a_2,...,a_n\}$ bases para $V$ y $W$ respectivamente. Entonces existe una transformación lineal $T:V\rightarrow W$ tal que $T(b_i)=a_i$ con $i=1,2,...,n$. Luego, como $\langle T(\beta)\rangle =Im(T)=\langle \alpha\rangle =W$, entonces $T$ es sobreyectiva, y como $V$ y $W$ tienen la misma dimensión, se cumple que también $T$ es inyectiva. Por lo tanto $V$ es isomorfo a $W$.
\end{proof}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales sobre un campo $K$. El conjunto de todas las transformaciones lineales de $V$ a $W$ es $L(V,W)$. Si $V=W$ escribimos simplemente $L(V)$, que sería el conjunto de todos los operadores lineales en $V$.
\end{definition}

\begin{proposition}
Si $T,U \in L(V,W)$, entonces $T+U \in L(V,W).$
\end{proposition}

\begin{proof}
Sean $T,U \in L(V,W)$; $v,s \in V$ y $c \in K$. Entonces
\[
\begin{split}
(T+U)(cv+s) &= T(cv+s)+U(cv+s) \\
&=cT(v)+T(s)+cU(v)+U(s) \\
&=cT(v)+cU(v)+T(s)+U(s) \\
&=c[T(v)+U(v)]+T(s)+U(s) \\
&=c[(T+U)(v)]+(T+U)(s) \\
\end{split}
\]
\end{proof}

\begin{proposition}
Si $T\in L(V,W)$ y $a\in K$, entonces $(aT)\in L(V,W)$.
\end{proposition}

\begin{proof}
Sean $T\in L(V,W)$; $v,s\in V$ y $a,c\in K$. Entonces
\[
\begin{split}
(aT)(cv+s) &=aT(cv+s) \\
&=a(T(cv)+T(s)) \\
&=caT(v)+aT(s) \\
&=c(aT)(v)+(aT)(s)
\end{split}
\]
\end{proof}

\begin{theorem}
El conjunto $L(V,W)$ con las operaciones de suma de funciones y multiplicación por un escalar en $K$, es un $K-$espacio vectorial.
\end{theorem}

\begin{proof}
Definiendo a la transformación nula como $T_0(v)=0$, $\forall v \in V$ y con las proposiciones anteriores, la demostración es bastante sencilla.
\end{proof}

\begin{theorem}
Si $V$, $W$ y $Z$ son espacios vectoriales y $T:V \rightarrow W$, $U:W \rightarrow Z$ transformaciones lineales, entonces $U\circ T:V \rightarrow Z$ es una transformación lineal.
\end{theorem}

\begin{proof}
Sean $v,s\in V$ y $c\in K$. Entonces
\[
\begin{split}
(U\circ T)(cv+s) &=U(T(cv+s)) \\
&=U(cT(v)+T(s)) \\
&=U(cT(v))+U(T(s)) \\
&=cU(T(v))+U(T(s)) \\
&=c(U \circ T)(v)+(U\circ T)(s)
\end{split}
\]
\end{proof}

%Agregar definición de k-algebra y su teorema

\subsection{Representación matricial de una transformación lineal}

\begin{definition}
Sea $V$ un espacio vectorial dimensionalmente finito. Una base ordenada para $V$ es una base para $V$ establecida con un orden especifico; es decir, una secuencia finita de elementos de $V$ que son linealmente independientes y que generan a $V$.
\end{definition}

\begin{definition}
Sea $\beta =\{ b_1,b_2,...,b_n\}$ una base ordenada de un espacio vectorial $V$. Para $v\in V$ definimos el vector coordenado de $v$ relativo a $\beta$, denotado por $[ v ]_{\beta}$, mediante
\[
[v]_{\beta}=\left( \begin{array}{c} v_1\\\vdots\\ v_n\\ \end{array} \right)
\]
donde
\[
v=\sum_{i=1}^{n}v_i b_i
\]
\end{definition}

\begin{definition}
Sean $V$ y $W$ dos espacios vectoriales con bases ordenadas $\beta=\{b_1,b_2,...,b_n\}$ y $\gamma=\{y_1,y_2,...,y_m\}$ respectivamente. Sea $T:V \rightarrow W$ lineal. Denotamos a la matriz de $m\times n$ que representa a $T$, como $[T]_{\beta} ^{\gamma}$; y se define por $([T]_{\beta} ^{\gamma})_{ij}=v_{ij}$, tal que
\[
T(b_j)=\sum_{i=1}^{m}v_{ij}y_i \ \ para \ \ j=1,2,...,n.
\]
Si $\beta=\gamma$ escribimos simplemente $[T]_{\beta}$. Nótese que la representación matricial de un operador lineal es una matriz cuadrada.
\end{definition}

\begin{proposition}
Sean $V$ y $W$ espacios vectoriales dimensionalmente finitos con bases $\beta$ y $\gamma$ respectivamente, y sean $U,T:V \rightarrow W$ transformaciones lineales. Entonces $[T+U]_{\beta} ^{\gamma}=[T]_{\beta} ^{\gamma} +[U]_{\beta} ^{\gamma}$ y $[cT]_{\beta} ^{\gamma} = c[T]_{\beta} ^{\gamma}$.
\end{proposition}

\begin{proof}

\end{proof} 

\begin{theorem}
Sean $V$ y $W$ espacios vectoriales con dimensiones $n$ y $m$, respectivamente, y sean $\beta$ y $\gamma$ bases ordenadas para $V$ y $W$, respectivamente. Entonces la función $\Phi:L(V,W)\rightarrow M_{m\times n}(K)$, definida por $\Phi (T)=[T]_{\beta} ^{\gamma}$ para $T\in L(V,W)$ es un isomorfismo.
\end{theorem}

\begin{proof}
Por la proposición anterior tenemos que $\Phi$ es lineal. Mostremos que $\Phi$ es biyectiva. Sean $\beta=\{b_1,b_2,...,b_n\}$ y $\gamma=\{y_1,y_2,...,y_m\}$. Luego, supongamos que $T\in L(V,W)$ y que $\Phi (T)=0_{M_{m\times n}(K)}$, entonces, para cada $j$ tenemos $T(b_j)=0y_1+0y_2+...+0y_m=0_W$. Por lo tanto $T=T_0$, por lo que $ker(\Phi)=\{T_0\}$, y en consecuencia $\Phi$ es inyectiva. 

Probemos ahora que $\Phi$ es sobreyectiva. 	Sea $A\in M_{m\times n}(K)$. Entonces existe $T\in L(V,W)$ tal que $T(b_j)=\sum_{i=1}^{m}A_{ij}y_{i}$ para $1\leq j\leq n$. Entonces $[T]_{\beta} ^{\gamma}=A$, y por lo tanto $\Phi(T)=A$, es sobreyectiva. Así, $\Phi$ es biyectiva y en consecuencia invertible, por lo que $\Phi$ es un isomorfismo.
\end{proof}

\begin{corollary}
Sean $V$ y $W$ espacios vectoriales con dimensiones $n$ y $m$, respectivamente. Entonces $dim(L(V,W))=mn$ 
\end{corollary}

\begin{proof}
Por el isomorfismo $\Phi$ se tiene que $dim(L(V,W))=dim(M_{m\times n}(K))$, y puesto que $dim(M_{m\times n}(K))$, se concluye que $dim(L(V,W))=mn$.
\end{proof}

\begin{proposition}
Sean $V$, $W$ y $Z$ espacios vectoriales dimensionalmente finitos con bases ordenadas $\alpha$, $\beta$ y $\gamma$, respectivamente. Sean $T:V \rightarrow W$ y $U:W \rightarrow Z$ transformaciones lineales. Entonces $[U\circ T]_{\alpha} ^{\gamma}=[U]_{\beta} ^{\gamma} [T]_{\alpha} ^{\beta}$.
\end{proposition}

\begin{proof}

\end{proof}

%Agregar teorema 2.21
\subsection{Matriz de cambio de coordenadas y matrices similares.}
\begin{definition}
Sean $\beta$ y $\beta '$ dos bases ordenadas para un espacio vectorial $V$ dimensionalmente finito. La matriz de cambio de base de coordenadas es $Q=[I_V]_{\beta '} ^{\beta}$.
\end{definition}

%Agregar antes teorema 2.16 como lema

\begin{theorem}
Sean $\beta$ y $\beta '$ dos bases ordenadas para un espacio vectorial $V$ dimensionalmente finito y $Q=[I_V]_{\beta '} ^{\beta}$ la matriz de cambio de base de coordenadas. Entonces \newline
a) $Q$ es invertible. \newline
b) Para toda $v\in V$, $[v]_{\beta}=Q[v]_{\beta'}$.
\end{theorem}

\begin{proof} %Correguir
Para el inciso a), tenemos que $I_V$ es invertible, por tanto al aplicarle el isomorfismo $\Phi$ a la inversa de $I_V$ obtenemos a la inversa de $Q$. Por otra parte, para el inciso b), tenemos que para toda $v\in V$, $[v]_{\beta}=[I_{V}(v)]_{\beta}=[I{V}]_{\beta '} ^{\beta} [v]_{\beta'}=Q[v]_{\beta'}$.
\end{proof}

\begin{definition}
Sean $A$ y $B$ matrices de $n \times n$ con entradas en el campo $K$. Decimos que $B$ es similar a $A$ si existe una matriz invertible $Q\in M_{n \times n}(K)$ tal que $B=Q^{-1}AQ$.
\end{definition}

\begin{theorem}
La relación de ser similar en el conjunto de matrices cuadradas es una relación de equivalencia.
\end{theorem}

\begin{proof}

\end{proof}

\chapter{Diagonalización}
\subsection{Eigenvalores y eigenvectores}

\begin{definition}
Se dice que un operador lineal $T$ sobre un espacio vectorial dimensionalmente finito $V$ es diagonalizable su existe una base ordenada $\beta$ para $V$ tal que $[T]_{\beta}$ sea una matriz diagonal. Por otra parte, una matriz cuadrada $A$ decimos que es diagonalizable si $A$ es similar a una matriz diagonal.
\end{definition}

\begin{theorem}
Sea $T$ un operador lineal sobre un espacio vectorial dimensionalmente finito $V$. Los siguientes incisos son equivalentes:
 \newline \newline
a) $T$ es diagonalizable. \newline
b) Existe una base ordenada $\beta$ para $V$ tal que la matriz $[T]_{\beta}$ es diagonalizable. \newline
c) La matriz $[T]_{\gamma}$ es diagonalizable para cualquier base ordenada $\gamma$ para $V$.
\end{theorem}

\begin{proof}
Si $T$ es diagonalizable, entonces existe una base ordenada $\beta$ para $V$ tal que $[T]_{\beta}$ es una matriz diagonal. Entonces $[T]_{\beta}$ es trivialmente diagonalizable, por lo que $a)$ implica $b)$. 

Sea $\beta$ una base ordenada para $V$ tal que $[T]_{\beta}$ es diagonalizable. Entonces $[T]_{\beta}$ y $[T]_{\gamma}$ son similares. Luego, si $[T]_{\beta}$ es similar a una matriz diagonal, también $[T]_{\gamma}$ lo será por la transitividad de la relación de similitud. Y entonces $[T]_{\gamma}$ es diagonalizable, demostrando que $b)$ implica $c)$.

Finalmente, si $[T]_{\gamma}$ es diagonalizable, existe una matriz diagonal $D$ similar a $[T]_{\gamma}$. Luego, existe una base ordenada $\beta '$ para $V$ tal que  $[T]_{\beta '}=D$. Por tanto, $T$ es diagonalizable y así $c)$ implica a $a)$.
\end{proof}

\begin{corollary}
Una matriz $A$ es diagonalizable si y solo si $L_{A}$ es diagonalizable.
\end{corollary}

\begin{theorem}
Sea $T$ un operador lineal sobre un espacio vectorial dimensionalmente finito $V$. Entonces $T$ es diagonalizable si y solo si existe una base ordenada $\beta =\{b_1,b_2,...,b_n\}$ para $V$ y escalares $\lambda_1, \lambda_2, ..., \lambda_n$ (no necesariamente distintos) tales que $T(b_j)=\lambda_j b_j$, para $1\leq j \leq n$. Bajo estas circunstancias 
\[
[T]_{\beta}=
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0\\
0 & \lambda_2 & \cdots & 0\\
\vdots & \vdots &   & \vdots\\
0 & 0 & \cdots & \lambda_{n}
\end{pmatrix}
\]
\end{theorem}

\begin{proof}

\end{proof}

\end{document}
