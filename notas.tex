\documentclass{book}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[mathscr,mathcal]{euscript}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{shapepar}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{shapepar}
\newtheorem{theorem}{Teorema}
\newtheorem{proposition}{Proposición}
\newtheorem{definition}{Definición}
\newtheorem{axiom}{Axioma}
\newtheorem{corollary}{Corolario}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{remark}{Remark}

\title{Notas de Álgebra Lineal}
\author{Carlos Francisco Flores Galicia.}
\date{}

\begin{document}

\maketitle

\chapter{Espacios vectoriales.}
\subsection{Espacios vectoriales}
\subsection{Subespacios vectoriales}
\subsection{Combinaciones lineales}

\begin{definition}
Sea $V$ un espacio vectorial y $S \subseteq V$, $S \neq \emptyset$. Un vector $v \in V$ es combinación lineal de elementos de $S$, si existe un conjunto finito $\{s_1,s_2,...,s_n\}\subseteq S$ y escalares $a_1,a_2,...a_n \in K$ tal que $v=a_1 s_1+a_2 s_2+...+a_n s_n$. Se dice también que $v$ es combinación lineal de $\{s_1,s_2,...,s_n\}$.
\end{definition}

\begin{definition}
Sea $V$ un espacio vectorial y $S=\{ s_1,s_2,...,s_n \} \subseteq V$. El conjunto generado por $S$ es $\langle S \rangle = \{a_1 s_1+a_2 s_2+...+a_n s_n : a_1,a_2,...a_n \in K \}$. Esto es, el conjunto generado por $S$ es el conjunto de todas las combinaciones lineales de los elementos de $S$.
\end{definition}

\begin{definition}
$\langle \emptyset \rangle=\{0_V\}$
\end{definition}

\begin{theorem}
Sea $V$ un espacio vectorial y $S \subseteq V$, $S \neq \emptyset$, entonces $\langle S \rangle \leq V$ y $\langle S \rangle$ es el subespacio de $V$ más pequeño que contiene a $S$ (es decir, que $\langle S \rangle$ es un subconjunto de todos los subespacios de $V$ que contienen a $S$).
\end{theorem}
\begin{proof}
Probemos primero que $\langle S \rangle \leq V$. Como $S \neq \emptyset$, al menos $0_V \in \langle S \rangle$. Luego, sean $u,v \in \langle S \rangle$, por tanto $u$ y $v$ son combinaciones lineales de elementos de $S$, de manera que existen $s_1,s_2,...s_n,t_1,t_2,...,t_n \in S$ tales que $v=a_1s_1+...+a_ns_n$ y $u=b_1t_1+...+b_nt_n$, con $a_1,...a_n,b_1,...,b_n \in K$. Ahora bien, es claro que $v+u=a_1s_1+...+a_ns_n+b_1t_1+...+b_nt_n$ y $cu=cb_1t_1+...+cb_nt_n$ pertenecen a $\langle S \rangle$, para cualquier $c \in K$. Por lo tanto $\langle S \rangle \leq V$.\newline \newline
Por otra parte, sea $U$ un subespacio de $V$ que contiene a $S$. Sea $v \in \langle S \rangle$, entonces $v=a_1s_1+...+a_ns_n$, con $a_1,...,a_n \in K$ y $s_1,...,s_n \in S$, además como $S \subseteq U$ entonces $v=a_1s_1+a_2s_2+...+a_ns_n \in U$, pues los subespacios vectoriales son cerrados bajo la suma y bajo el producto por escalares. Por tanto tenemos que si $v \in \langle S \rangle$ entonces $v \in U$, así que $\langle S \rangle \subseteq U$.
\end{proof}

\begin{definition}
Sea $S \subseteq V$. Decimos que $S$ genera a $V$ si $\langle S \rangle = V$. También podemos decir que los elementos de $S$ generan a $V$.
\end{definition}

\subsection{Dependencia e independencia lineal.}

\begin{definition}
Sea $S=\{s_1,s_2,...,s_n\}\subseteq V$. Decimos que $S$ es linealmente dependiente si existe $s \in S$ tal que $s \in \langle S-\{s\} \rangle$.
\end{definition}

\begin{theorem}
Sea $S=\{s_1,s_2,...,s_n\}\subseteq V$. $S$ es linealmente dependiente si y solo si existen $a_1,a_2,...,a_n \in K$ tal que $a_1s_1+a_2s_2+...+a_ns_n=0_V$ y $a_1,a_2,...,a_n $ no son todos cero.
\end{theorem}

\begin{proof}
$\Rightarrow$ Supongamos que $S$ es linealmente dependiente, entonces existe $s \in S$ tal que $s \in \langle S-\{ s \} \rangle$, por tanto existen $s_1,s_2,...,s_n \in S-\{s\}$ y los escalares $a_1,a_2,...,a_n \in K$ tales que $s=a_1s_1+a_2s_2+...+a_ns_n$. Al sumar $-s$ en ambos lados de la expresión anterior obtenemos $0_V=-s+a_1s_1+a_2s_2+...+a_ns_n$, con lo cual se garantiza que no todos los escalares que multiplican a los vectores son cero, pues $-1$ multiplica a $s$.\newline \newline
$\Leftarrow$ Supongamos que existen $a_1,a_2,...,a_n \in K$, no todos cero, tales que $a_1s_1+a_2s_2+...+a_ns_n=0_V$. Puesto que no todos los escalares son cero, supongamos sin perdida de generalidad que $a_1\neq 0$, por tanto podemos multiplicar en ambos lados de la igualdad anterior por el escalar $\dfrac{1}{a_1}$. En consecuencia obtenemos $s_1+\dfrac{a_2}{a_1}s_2+...+\dfrac{a_n}{a_1} s_n=0_V$. Luego, al sumar $-s_1$ y multiplicar por $-1$ en ambos lados nos queda que $s_1=\left(-\dfrac{a_2}{a_1}\right) s_2+...+\left( - \dfrac{a_n}{a_1}\right) s_n$, esto es, que $s_1 \in \langle S -\{ s_1 \} \rangle$. Por lo tanto $S$ es linealmente dependiente.
\end{proof}

\begin{definition}
Sea $S \subseteq V$. Decimos que $S$ es linealmente independiente si y solo si no es linealmente dependiente.
\end{definition}

Por la equivalencia lógica $(P \Leftrightarrow \exists x (Q \wedge S)) \Leftrightarrow (\neg P \Leftrightarrow \forall x (Q\Rightarrow \neg S))$, el teorema anterior es equivalente a la siguiente proposición que enunciaremos como corolario.

\begin{corollary}
Sea $S \subseteq V$. $S$ es linealmente independiente si y solo si para todo $a_1,a_2,...,a_n \in K$ tal que si $a_1s_1+a_2s_2+...+a_ns_n=0_V$ entonces $a_1,a_2,...,a_n$ son todos cero.
\end{corollary}

\begin{proof}
Se sigue del teorema anterior y de la equivalencia lógica $(P \Leftrightarrow \exists x (Q \wedge S)) \Leftrightarrow (\neg P \Leftrightarrow \forall x (Q\Rightarrow \neg S))$.
\end{proof}

\begin{proposition}
Si $S \subseteq V$ y $0_V \in S$, entonces $S$ es linealmente dependiente.
\end{proposition}

\begin{proof}

\end{proof}

\begin{theorem}
El conjunto $\emptyset$ es linealmente independiente
\end{theorem}

\begin{proof}
Supongamos que $\emptyset$ es linealmente dependiente, entonces existe $s \in \emptyset$ tal que $s \in \langle \emptyset-\{s\} \rangle$. Como $s \in \emptyset$ entonces por definición del conjunto vacío se cumple que $s\neq s$ lo cual es una contradicción. Por lo tanto el conjunto $\emptyset$ es linealmente independiente.
\end{proof}

\begin{lemma}
Si $V$ es un $K-$espacio vectorial y $R\subseteq S\subseteq V$, entonces $\langle R \rangle \subseteq \langle S \rangle$.
\end{lemma}

\begin{proof}
Supongamos que $R\subseteq S\subseteq V$, y sea $r \in \langle R \rangle$, por lo tanto existe un subconjunto $\{r_1,r_2,...,r_m \} \subseteq R$ y los escalares $a_1,a_2,...a_m \in K$ tal que $r=a_1r_1,a_2r_2,...,a_mr_m$. Como $R\subseteq S$, entonces $r_1,r_2,...,r_m \in S$, de manera que $r=a_1r_1,a_2r_2,...,a_mr_m \in \langle S \rangle$, por lo tanto $\langle R \rangle \subseteq \langle S \rangle$.

\end{proof}

\begin{theorem}
Sea $V$ un $K-$espacio vectorial y sean $S_1\subseteq S_2\subseteq V$. Si $S_1$ es linealmente dependiente entonces $S_2$ también lo es.
\end{theorem}

\begin{proof}
Supongamos que $S_1$ es linealmente dependiente, entonces existe $s \in S_1$ tal que $s \in \langle S_1-\{s\} \rangle$. Luego, como $S_1\subseteq S_2$ entonces $S_1-\{s\} \subseteq S_2-\{s \}$, y por el lema anterior $\langle S_1-\{s \} \rangle \subseteq \langle S_2-\{s \} \rangle$, por lo que $s \in \langle S_2-\{s\} \rangle$, luego $S_2$ es linealmente dependiente.
\end{proof}

\begin{corollary}
Sea $V$ un $K-$espacio vectorial y sean $S_1\subseteq S_2\subseteq V$. Si $S_2$ es linealmente independiente entonces $S_1$ también lo es.
\end{corollary}

\begin{proof}
La demostración se sigue de hacer la contrapositiva del teorema anterior.
\end{proof}

\subsection{Bases y dimensiones.}

\begin{definition}
Sea $\beta \subseteq V$. Decimos que $\beta$ es una base para $V$ si y solo si $\beta$ es linealmente independiente y $\langle \beta \rangle=V$.
\end{definition}

\begin{theorem}

Sea $V$ un espacio vectorial y $\beta = \{b_1, b_2,...,b_n\}$ un subconjunto de $V$. Luego $\beta$ es una base para $V$ si y sólo si cada vector $v \in V$ puede ser expresado de manera única como una combinación lineal de vectores de $\beta$, es decir, puede ser expresado de la forma $v = a_1 b_1 + a_2 b_2 + ...+ a_n b_n$, para escalares únicos $a_1, a_2, ..., a_n \in K$.

\end{theorem}

\begin{proof}

$\Rightarrow$ Supongamos que $\beta$ es una base para $V$. Sea $v \in V$, entonces $v \in \langle \beta \rangle$, así que existen $a_1, a_2, ..., a_n \in K$ tal que $v = a_1 b_1 + a_2 b_2 + ...+ a_n b_n$. Ahora, supongamos que también $v = c_1 b_1 + c_2 b_2 + ...+ c_n b_n$, con $c_1,c_2,...,c_n \in K$. Entonces es claro que $v-v=0_V=(a_1 - c_1)b_1+(a_2 -c_2)b_2 +...+(a_n -c_n)b_n$. Ya que $\beta$ es linealmente independiente, $a_1 -c_1=0, a_2 -c_2=0, ..., a_n -c_n=0$, en consecuencia $a_1=c_1, a_2 =c_2, ..., a_n =c_n$, por lo tanto la representación de $v$ como combinación lineal de $\beta$ es única.\newline

$\Leftarrow$ Supongamos que cada vector $v \in V$ puede ser expresado como una combinación lineal de vectores de $\beta$ con los escalares únicos $a_1, a_2, ..., a_n \in K$. Por lo tanto $\langle \beta \rangle=V$. Probemos ahora que $\beta$ es linealmente independiente. Tenemos que el elemento $0_V$ puede ser expresado como $0_V=a_1b_1+a_2b_2+...+a_nb_n$, y puesto que esta manera es única, se tiene que cada escalar $a_1, a_2, ..., a_n$ debe ser $0$, por lo tanto $\beta$ es linealmente independiente, y en consecuencia $\beta$ es una base.

\end{proof}

\begin{theorem}
Si $V$ es un espacio vectorial y $S \subseteq V$ tal que $S$ es finito y genera a $V$, entonces existe $S' \subseteq S$ tal que $S'$ es una base para $V$.

\end{theorem}

\begin{proof}
Si $S= \emptyset$ o $S=\{0_V\}$ entonces $V=\{0_V\}$ y como $\emptyset$ es subconjunto de cualquier conjunto, entonces $S$ es una base para $V$. De lo contrario, $V$ tendrá al menos un elemento $v_1$ no nulo. Nótese que $\{v_1\}$ es un conjunto linealmente independiente. Continúese, si es posible, escogiendo elementos $v_2,v_3,...,v_r \in V$ tales que $\{ v_1,v_2,v_3,...,v_r \}$ sea linealmente independiente. Puesto que $S$ es finito, se llegará al punto en el que $S'=\{v_1,v_2,v_3,...,v_r\}$ sea un subconjunto de $S$ linealmente independiente, de manera que al agregar otro delemento de $S$ a $S'$, éste sea linealmente dependiente. Demostremos ahora que $S'$ es una base para $V$. Como $S'$ es linealmente independiente, basta mostrar que es generador de $V$, pero como $\langle S \rangle=V$, es suficiente demostrar que $S \subseteq \langle S' \rangle$. Sea $v \in S$. Si $v \in S'$, entonces $v \in \langle S' \rangle$. Por otro lado, si $v$ no está en $S'$, la anterior construcción mostraría que $S' \cup \{v\}$ es linealmente dependiente. Así, $v \in \langle S' \rangle$, y por tanto $S \subseteq \langle S' \rangle$.
\end{proof}

\begin{theorem}
Sea $V$ un espacio vectorial que tiene una base $\beta$ con exactamente $n$ elementos. Sea $S=\{s_1,s_2,...,s_m\} \subseteq V$ linealmente independiente con exactamente $m$ elementos, donde $m \leq n$. Entonces, existe un subconjunto $S_1 \subseteq \beta$ que contiene exactamente $n-m$ elementos tales que $\langle S \cup S_1 \rangle=V$. 
\end{theorem}

\begin{proof}
La demostración se hará por inducción sobre $m$. Si $m=0$, entonces $S=\emptyset$, y así $S_1=\beta$ satisface el teorema. Ahora, supongamos que que el teorema es cierto para $m$, tal que $m<n$, y demostremos que también se cumple para $m+1$. Sea $S=\{s_1,s_2,...,s_m,s_(m+1)\}$ un subconjunto de $V$ linealmente independiente, el cual contiene exactamente $m+1$ elementos. Puesto que $S=\{s_1,s_2,...,s_m\}$ también es linealmente independiente, por la hipótesis de inducción se tiene que existe un subconjunto $\{b_1,b_2,...,b_(n-m)\}$ de $\beta$ tal que $\{s_1,s_2,...,s_m\} \cup \{b_1,b_2,...,b_{n-m}\}$ genera a $V$. En consecuencia existirán escalares $a_1,...,a_m,c_1,c_2,...,c_{n-m} \in K$ tales que $y_{m+1}=a_1s_1+...+a_ms_m+c_1b_1+c_2b_2+...+c_{n-m}b_{n-m}$. Notemos que algún $b_i$, tal como $b_1$, es no nulo, de lo contrario $y_{m+1}$ es una combinación lineal de $S=\{s_1,s_2,...,s_m,s_(m+1)\}$ y eso contradice el hecho de que $S$ es linealmente independiente. Al despegar $b_1$ se obtiene $b_1=(-c_1^{-1}a_1)s_1+...+(-c_1^{-1}a_m)s_m-(-c_1^{-1})s_{m+1}+(-c_1^{-1}c_2)s_2+...+(-c_1^{-1}c_{n-m})s_{n-m}$. Entonces $b_1 \in \langle \{s_1,...,s_m,s_{m+1},b_2,...,b_{n-m} \} \rangle$, pero como $s_1,...,s_m,b_2,...,b_{n-m}$ son elementos de $ \langle \{s_1,...,s_m,s_{m+1},b_2,...,b_{n-m} \} \rangle$, se tendrá que $\{s_1,...,s_m,b_2,...,b_{n-m}\} \subseteq  \langle \{s_1,...,s_m,s_{m+1},b_2,...,b_{n-m} \} \rangle$. Por lo tanto $\langle \{s_1,...,s_m,s_{m+1},b_2,...,b_{n-m} \} \rangle = V$. Luego, al escoger $S_1=\{s_2,...,s_{n-m} \}$ demuestra que el teorema es cierto para $m+1$. Esto completa la demostración.
\end{proof}

\begin{corollary}
Sea $V$ un espacio vectorial que tiene una base $\beta$ que contenga exactamente $n$ elementos. Entonces, cualquier subconjunto linealmente independiente de $V$ que contenga exactamente $n$ elementos es una base de $V$.
\end{corollary}

\begin{proof}
Sea $S=\{s_1,s_2,..,s_n\}$ un conjunto de $V$ linealmente independiente que contiene exactamente $n$ elementos. Por el teorema anterior, existe $S_1 \subseteq \beta$ que contiene $n-n=0$ elementos tal que $\langle S \cup S_1\rangle =V$. Obviamente $S_1=\emptyset$; luego, $\langle S \rangle =V$. Como $S$ es también linealmente independiente, $S$ es una base para $V$.
\end{proof}

\begin{corollary}
Sea $V$ un espacio vectorial que tiene una base $\beta$ que contenga exactamente $n$ elementos. Entonces, cualquier subconjunto de $V$ que contenga más de $n$ elementos es linealmente dependiente.
\end{corollary}

\begin{proof}
Sea $S\subseteq V$ que contiene más de $n$ elementos. 	Supongamos que $S$ es linealmente independiente. Sea $S_1 \subset S$ con exactamente $n$ elementos . Entonces, por el corolario anterior $S_1$ es una base para $V$. Como $S_1$ es subconjunto propio de $S$, podemos tomar un elemento $s\in S$ tal que $s\notin S_1$. Como $S_1$ es base para $V$, $s\in \langle S_1 \rangle =V$. Luego, $S_1 \cup \{s\}$ es linealmente dependiente. Pero $S_1 \cup \{s\} \subseteq S$; luego, $S$ es linealmente dependiente, y esto es una contradicción. Por lo tanto, $S$ es linealmente dependiente.

\end{proof}

\begin{corollary}
Sea $V$ un espacio vectorial que tiene una base $\beta$ con exactamente $n$ elementos. Entonces, toda base para $V$ contendrá exactamente $n$ elementos.
\end{corollary}

\begin{proof}
Sea $S$ una base de $V$. Como $S$ es linealmente independiente tendrá como máximo $n$ elementos. Supongamos que $S$ contiene exactamente $m$ elementos; luego, $m\leq n$. Pero además, $S$ es una base de $V$ y $\beta$ es linealmente independiente. Entonces, aplicamos el corolario anterior intercambiando los papeles de $\beta$ y $\S$ para dar $n \leq m$. Luego $m=n$.
\end{proof}

\begin{definition}
Un espacio vectorial $V$ es dimensionalmente finito  si tiene una base cuya cardinalidad es un número finito. La cardinalidad de una base de $V$ es la dimension de $V$, y se denota por $dim(V)$. Si un espacio vectorial no es dimensionalmente finito, se llama dimensionalmente infinito.
\end{definition}

\begin{theorem}
Sea $U\leq V$ y $dim(V)=n$. Entonces, $W$ es dimensionalmente finito y $dim(W)\leq n$. Además, si $dim(W)=n$, entonces $W=V$.
\end{theorem}

\begin{proof}
Si $W=\{0_w\}$, $W$ es dimensionalmente finito y $dim(W)=0\leq n$. De otra manera, existe un elemento no nulo $w_1\in W$, y así $\{w_1\}$ es linealmente independiente. Continuando en esta forma, tómese elementos $w_1,w_2,...,w_k\in W$ tales que $\{w_1,w_2,...,w_k \}$ sea linealmente independiente. Este proceso debe terminar en una etapa donde $\{w_1,w_2,...,w_k \}$ sea linealmente independiente pero de manera que al añadir cualquier elemento de $W$ se tenga un conjunto linealmente dependiente. Entonces, $W$ tiene una base finita que contiene no más de $n$ elementos; esto es, $dim(W)\leq n$. Si $dim(W)=n$, entonces una base para $W$ seria un subconjunto de $V$ linealmente independiente que contuviera $n$ elementos, y esto implicaría que la base para $W$ es también una base para $V$, por tanto $W=V$.
\end{proof}

\chapter{Matrices y transformaciones lineales.}
\subsection{Matrices}
\subsection{Transformaciones lineales.}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales sobre un campo $K$. Una función $T:V\rightarrow W$ se llama transformación lineal de $V$ en $W$ si para todo $u,v\in V$ y para todo $c\in K$, se cumple que $T(u+v)=T(u)+T(v)$ y $T(cv)=cT(v)$.

\end{definition}

\begin{proposition}

Sean $u,v\in V$ y $c\in K$. Luego, $T:V\rightarrow W$ es una transformación lineal si y solo si $T(cu+v)=cT(u)+T(v)$.

\end{proposition}

\begin{proof}

Como $T$ es transformación lineal, entonces $T(cu+v)=T(cu)+T(v)=cT(u)+T(v)$. La segunda implicación es análoga a la primera.

\end{proof}

\begin{proposition}

Si $T:V\rightarrow W$ es una transformación lineal, entonces $T(0_V)=0_W$.

\end{proposition}

\begin{proof}
\[ 0_V=0_V+0_V\]
\[T(0_V)=T(0_V+0_V)\]
\[T(0_V)=T(0_V)+T(0_V)\]

Por otra parte, como $T(0_V)\in W$, entonces $T(0_V)=T(0_V)+0_W$. Luego

\[T(0_V)+T(0_V)=T(0_V)+0_W\] 
\[T(0_V)=0_W\]
\end{proof} 

\begin{definition}
Sean $V$ y $W$ espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. El espacio nulo o kernel de $T$ es  
\[ker(T)=\{x\in V \ |\ T(x)=0_W \}\]
\end{definition}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. La imagen de $T$ es un subconjunto de $W$ que se define como 
\[Im(T)=\{T(x) \ |\ x\in V \}\]
\end{definition}

\begin{theorem}
Si $V$ y $W$ son espacios vectoriales y $T:V\rightarrow W$ una transformación lineal, entonces $ker(T)$ y $Im(T)$ son subespacios de $V$ y $W$, respectivamente.
\end{theorem}

\begin{proof}
Como $T(0_V)=0_W$, tenemos que $0_V\in ker(T)$. Sean $v_1,v_2 \in ker(T)$ y $c\in K$. Entonces $T(v_1+v_2)=T(v_1)+T(v_2)=0_W+0_W=0_W$, y $T(cv_1)=cT(v_1)=c0_W=0_w$. Por lo tanto $v_1,v_2\in ker(T)$ y $cv_1\in ker(T)$. Luego $ker(T)\leq V$

Por otra parte, Como $T(0_V)=0_W$, tenemos que $0_W\in Im(T)$. Ahora, sean $w_1,w_2 \in Im(T)$ y $c\in K$. Entonces existen $v_1$ y $v_2$ tales que $T(v_1)=w_1$ y $T(v_2)=w_2$. Así, $T(v_1+v_2)=T(v_1)+T(v_2)=w_1+w_2$, y $T(cv_1)=cT(v_1)=cw_1$. Por lo tanto, $v_1,v_2\in Im(T)$ y $cv_1\in Im(T)$. Luego $Im(T)\leq W$.
\end{proof}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. La nulidad de $T$ es $nul(T)=dim(ker(T))$.
\end{definition}

\begin{definition}
Sean $V$ y $W$ espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. El rango de $T$ es $R(T)=dim(Im(T))$.
\end{definition}

\begin{theorem}
Si $V$ y $W$ son espacios vectoriales, $V$ de dimension finita y $T:V\rightarrow W$ una transformación lineal, entonces $nul(T)+R(T)=dim(V)$.
\end{theorem}

\begin{proof}
Supongamos que $dim(V)=n$, y sea $\{b_1,b_2,...,b_k\}$ una base para $nul(T)$. Entonces podemos extender a $\{b_1,b_2,...,b_k\}$ para que sea una base para $V$. Supongamos que esa extensión es $\beta=\{b_1,b_2,...,b_n\}$. Demostraremos que el conjunto $S=\{T(b_{k+1}),...,T(b_n)\}$ es una base para $R(T)$.

Primero demostremos que $S$ genera a $R(T)$. 
\end{proof}

\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. La transformación $T$ es inyectiva si y solo si $ker(T)=\{0_V\}$
\end{theorem}

\begin{proof}
Supongamos que $T$ es inyectiva y que $v\in ker(T)$. Entonces $T(v)=T(0_V)=0_W$, y por lo tanto $v=0_V$. Así, $ker(T)=\{0_V\}$. Ahora supongamos que $ker(T)=\{0_V\}$ y que $T(v_1)=T(v_2)$. Entonces $T(v_1)-T(v_2)=T(v_1-v_2)=0_W$, por lo tanto $x-y\in ker(T)=\{0_V\}$ y en consecuencia $x-y=0_V$, esto es, $x=y$. Por lo tanto $T$ es inyectiva.
\end{proof}

\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales de dimensiones finitas e iguales y $T:V\rightarrow W$ una transformación lineal. Entonces $T$ es inyectiva si y solo si es sobreyectiva.
\end{theorem}

\begin{proof}
$\Rightarrow$ Supongamos que $T$ es inyectiva, entonces $ker(T)=\{0_V\}$, por lo tanto $\emptyset$ es una base para $ker(T)$, y así $nul(T)=0$, por lo tato $0+R(T)=dim(V)=dim(W)$, luego $dim(Im(T))=dim(W)$, y en consecuencia $Im(T)=W$.\newline 
$\Leftarrow$ Supongamos que $T$ es sobreyectiva pero no es inyectiva. Entonces $nul(T)>0$, pues $\emptyset$ no es base para $ker(T)$. Como $T$ es sobreyectiva, entonces $Im(T)=W$, y así $R(T)=dim(W)=dim(V)$. Luego, $nul(T)+R(T)>dim(V)$, lo cual es una contradicción, por lo tanto $T$ es inyectiva.
\end{proof}

\begin{theorem}
Sean $V$ y $W$ dos espacios vectoriales y $T:V\rightarrow W$ una transformación lineal. Entonces $T$ es inyectiva si y solo si $T$ lleva subconjuntos linealmente independientes de $V$ a subconjuntos linealmente independientes de $W$.
\end{theorem}

\begin{proof}

\end{proof}

\begin{theorem}
Sean $V$ y $W$ espacios vectoriales y supóngase que $\{b_1,b_2,...,b_n\}$ es una base para $V$. Para cualquier subconjunto $\{w_1,w_2,...,w_n\}\subset W$ existe exactamente una transformación lineal $T:V\rightarrow W$ tal que $T(b_i)=w_i$ para $i=1,2,...,n$.
\end{theorem}

\begin{proof}

\end{proof}

\subsection{Matrices}

\subsection{Representación matricial de una transformación lineal}

\end{document}
